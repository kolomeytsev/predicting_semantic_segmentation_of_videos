{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import argparse\n",
    "import pyflow\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import misc\n",
    "import time\n",
    "from IPython import display\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size =  4209\n",
      "val_data_size =  1999\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(names_dir):\n",
    "        # get video names\n",
    "        file = open(names_dir,'r') \n",
    "        names = file.readlines()\n",
    "        \n",
    "        # get the images\n",
    "        imgs = []\n",
    "        maps = []\n",
    "        for name in names:  \n",
    "            maps.append(sorted(glob(os.path.join('data/DAVIS/Annotations/480p', name[:-1],  '*'))))\n",
    "            imgs.append(sorted(glob(os.path.join('data/DAVIS/JPEGImages/480p', name[:-1],  '*'))))\n",
    "        return imgs, maps\n",
    "    \n",
    "train_data = get_dataset('data/DAVIS/ImageSets/2017/train.txt')\n",
    "val_data = get_dataset('data/DAVIS/ImageSets/2017/val.txt')\n",
    "\n",
    "train_data_size = 0\n",
    "val_data_size = 0\n",
    "\n",
    "for i in range(len(train_data[0])):\n",
    "    train_data_size += len(train_data[0][i])\n",
    "    \n",
    "for i in range(len(val_data[0])):\n",
    "    val_data_size += len(val_data[0][i])\n",
    "    \n",
    "print(\"train data size = \",train_data_size)\n",
    "print(\"val_data_size = \",val_data_size)\n",
    "\n",
    "train_length = np.zeros(len(train_data[0]),dtype = int)\n",
    "val_length = np.zeros(len(val_data[0]),dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pre-load dataset to speed up further data preparations\n",
    "def_h = 480\n",
    "def_w = 840\n",
    "\n",
    "train_counter = 0\n",
    "val_counter = 0\n",
    "n_channels = 3\n",
    "\n",
    "Train_data_seg = np.zeros((train_data_size,def_h,def_w), dtype = np.uint8)\n",
    "Val_data_seg = np.zeros((val_data_size,def_h,def_w), dtype = np.uint8)\n",
    "\n",
    "Train_data_img = np.zeros((train_data_size,def_h,def_w,n_channels), dtype = np.uint8)\n",
    "Val_data_img = np.zeros((val_data_size,def_h,def_w,n_channels), dtype = np.uint8)\n",
    "\n",
    "for i in range(len(train_data[1])):\n",
    "    #number of frames in this particular video\n",
    "    train_length[i] = len(train_data[1][i])\n",
    "    for j in range(train_length[i]):\n",
    "        seg = np.asarray(Image.open(train_data[1][i][j])).copy()\n",
    "        img = np.asarray(Image.open(train_data[0][i][j])).copy()\n",
    "        h,w = seg.shape\n",
    "        #we want to work only with 480 x 848 images\n",
    "        if w != def_w:\n",
    "            step = (w-def_w) // 2\n",
    "            if (w - def_w) % 2 == 0:\n",
    "                seg = seg[:,step:w-step]\n",
    "                img = img[:,step:w-step,:]\n",
    "            else:\n",
    "                seg = seg[:,step+1:w-step]\n",
    "                img = img[:,step+1:w-step,:]\n",
    "        #binary segmentation\n",
    "        seg[seg > 0] = 1\n",
    "        Train_data_seg[train_counter,:,:] = seg  \n",
    "        Train_data_img[train_counter,:,:,:] = img \n",
    "        train_counter += 1\n",
    "\n",
    "for i in range(len(val_data[1])):\n",
    "    #number of frames in this particular video\n",
    "    val_length[i] = len(val_data[1][i])\n",
    "    for j in range(val_length[i]):\n",
    "        seg = np.asarray(Image.open(val_data[1][i][j])).copy()\n",
    "        img = np.asarray(Image.open(val_data[0][i][j])).copy()\n",
    "        h,w = seg.shape\n",
    "        #we want to work only with 480 x 848 images\n",
    "        if w != def_w:\n",
    "            step = (w-def_w) // 2\n",
    "            if (w - def_w) % 2 == 0:\n",
    "                seg = seg[:,step:w-step]\n",
    "                img = img[:,step:w-step,:]\n",
    "            else:\n",
    "                seg = seg[:,step+1:w-step]\n",
    "                img = img[:,step+1:w-step,:]\n",
    "        #binary segmentation\n",
    "        seg[seg > 0] = 1\n",
    "        Val_data_seg[val_counter,:,:] = seg  \n",
    "        Val_data_img[val_counter,:,:,:] = img \n",
    "        val_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0   82  172  247  334  405  485  565  654  717  801  853  913  975 1051\n",
      " 1076 1162 1245 1309 1400 1480 1560 1640 1715 1775 1843 1923 2023 2088 2161\n",
      " 2213 2283 2353 2433 2503 2563 2606 2652 2722 2760 2810 2900 2935 3015 3106\n",
      " 3181 3249 3329 3395 3443 3534 3605 3660 3720 3790 3866 3946 4005 4070 4137]\n",
      "[   0   69  119  199  283  373  448  488  592  682  742  808  860  910 1000\n",
      " 1078 1128 1209 1243 1293 1340 1389 1439 1518 1558 1638 1738 1817 1860 1900]\n"
     ]
    }
   ],
   "source": [
    "train_length_cum = np.zeros(len(train_data[0]),dtype = int)\n",
    "val_length_cum = np.zeros(len(val_data[0]),dtype = int)\n",
    "\n",
    "train_length_cum = np.cumsum(train_length) - train_length\n",
    "val_length_cum = np.cumsum(val_length) - val_length\n",
    "\n",
    "print(train_length_cum)\n",
    "print(val_length_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_hist(pred, label, n=2):\n",
    "    k = (label >= 0) & (label < n)\n",
    "    return np.bincount(n * label[k] + pred[k], minlength=n ** 2).reshape(n, n)\n",
    "#         n * label[k].astype(int) + pred[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "def per_class_iu(hist):\n",
    "    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
    "\n",
    "# def compute_metrics(pred,label,n_classes):\n",
    "#     \"\"\"\n",
    "#     pred,label = np.array, shape = (batch_size,H,W), entries \\in [0,n_classes-1]\n",
    "#     \"\"\"\n",
    "#     iou = np.zeros(n_classes,dtype = float)\n",
    "#     accuracy = []\n",
    "#     for i in range(pred.shape[0]):\n",
    "#         hist = fast_hist(pred[i,:,:],label[i,:,:],n_classes)\n",
    "#         iou += per_class_iu(hist)\n",
    "#         accuracy.append(np.diag(hist).sum()/hist.sum())\n",
    "#     iou = iou / pred.shape[0]\n",
    "#     mean_iou = iou.mean()\n",
    "#     mean_accuracy = np.mean(accuracy)\n",
    "#     return mean_accuracy,mean_iou,iou\n",
    "\n",
    "def compute_metrics(pred, label, n_classes):\n",
    "    \"\"\"\n",
    "    pred, label = np.array, shape = (batch_size, H, W), entries \\in [0, n_classes - 1]\n",
    "    \"\"\"\n",
    "    iou = np.zeros(n_classes, dtype = float)\n",
    "    accuracy = []\n",
    "    hist = fast_hist(pred, label, n_classes)\n",
    "    iou += per_class_iu(hist)\n",
    "    accuracy.append(np.diag(hist).sum() / hist.sum())\n",
    "    iou = iou\n",
    "    mean_iou = iou.mean()\n",
    "    mean_accuracy = np.mean(accuracy)\n",
    "    return mean_accuracy, mean_iou, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches_AR_all_indices(dataset, cumul, n_frames=1, \n",
    "                                    n_out=1, step=1):\n",
    "    \"\"\"\n",
    "    generates start indices\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for vid in range(len(dataset[0])):\n",
    "        for start in range(0, len(dataset[0][vid]) - n_frames - n_out + 1, step):\n",
    "            start_ind = cumul[vid] + start\n",
    "            #end_ind = start_ind + n_frames + n_out\n",
    "            indices.append(start_ind)\n",
    "    res = {\n",
    "        \"indices\": indices, \"n_frames\": n_frames, \"n_out\": n_out\n",
    "    }    \n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_batches_AR_all(dataset, X_seg, indices_dict, batch_size=2):\n",
    "    start_indices = np.array(indices_dict[\"indices\"])\n",
    "    n_frames = indices_dict[\"n_frames\"]\n",
    "    n_out = indices_dict[\"n_out\"]\n",
    "    random_indices = np.random.permutation(np.arange(len(start_indices)))\n",
    "    for start in range(0, len(random_indices), batch_size):\n",
    "        batch_random_indices = random_indices[start: start + batch_size]\n",
    "        batch_start_indices = start_indices[batch_random_indices]\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        for out in range(n_out):\n",
    "            loc_X = []\n",
    "            loc_y = []\n",
    "            for b in range(batch_size):\n",
    "                if b >= len(batch_start_indices):\n",
    "                    break\n",
    "                start_ind = batch_start_indices[b]\n",
    "                seg = X_seg[start_ind+out:start_ind+n_frames+out+1,:,:]\n",
    "        \n",
    "                #seg = torch.from_numpy(seg).float()\n",
    "                \n",
    "                #n,h,w = seg.size()            \n",
    "              \n",
    "                loc_X.append(seg[:-1,:,:])\n",
    "                loc_y.append(seg[-1,:,:])\n",
    "            X_batch.append(np.stack(loc_X))\n",
    "            y_batch.append(np.stack(loc_y))\n",
    "        #note that it is 5-dim tensor, not 4-dim\n",
    "        yield np.stack(X_batch)[0][0], np.stack(y_batch)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 480, 840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data_seg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_out = 1\n",
    "n_classes = 2\n",
    "n_frames = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices = generate_batches_AR_all_indices(train_data, train_length_cum, n_frames, n_out)\n",
    "val_indices = generate_batches_AR_all_indices(val_data, val_length_cum, n_frames, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "number_of_batches_train = int(np.ceil(len(train_indices[\"indices\"]) / batch_size))\n",
    "number_of_batches_val = int(np.ceil(len(val_indices[\"indices\"]) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a427470ee934325a15357e77b999e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(666)\n",
    "\n",
    "acc = []\n",
    "miou = []\n",
    "iou = []\n",
    "\n",
    "for X_batch, labels in tqdm_notebook(generate_batches_AR_all(val_data, Val_data_seg, val_indices, batch_size=batch_size)):\n",
    "    im1 = X_batch[0]\n",
    "    im2 = X_batch[1]\n",
    "\n",
    "    im1 = im1.astype(float) / 1.\n",
    "    im2 = im2.astype(float) / 1.\n",
    "\n",
    "    im1 = np.expand_dims(im1, 2)\n",
    "    im2 = np.expand_dims(im2, 2)\n",
    "\n",
    "    im1 = np.concatenate((im1, im1, im1), axis=2)\n",
    "    im2 = np.concatenate((im2, im2, im2), axis=2)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Demo for python wrapper of Coarse2Fine Optical Flow')\n",
    "    parser.add_argument('-viz', dest='viz', action='store_true', help='Visualize (i.e. save) output of flow.')\n",
    "\n",
    "    # Flow Options:\n",
    "    alpha = 0.012\n",
    "    ratio = 0.75\n",
    "    minWidth = 20\n",
    "    nOuterFPIterations = 7\n",
    "    nInnerFPIterations = 1\n",
    "    nSORIterations = 30\n",
    "    colType = 0  # 0 or default:RGB, 1:GRAY (but pass gray image with shape (h,w,1))\n",
    "\n",
    "    s = time.time()\n",
    "    u, v, im2W = pyflow.coarse2fine_flow(\n",
    "        im1, im2, alpha, ratio, minWidth, nOuterFPIterations, nInnerFPIterations,\n",
    "        nSORIterations, colType)\n",
    "    flow = np.concatenate((u[..., None], v[..., None]), axis=2)\n",
    "\n",
    "    hsv = np.zeros(im1.shape, dtype=np.uint8)\n",
    "    hsv[:, :, 0] = 255\n",
    "    hsv[:, :, 1] = 255\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    im2W = np.einsum('ijk -> kij', im2W)[0]\n",
    "    im2W[im2W > 0] = 1.\n",
    "    im2W = im2W.astype(np.int32)\n",
    "\n",
    "    acc_loc, miou_loc, iou_loc = compute_metrics(im2W, labels, 2)\n",
    "    \n",
    "    acc.append(acc_loc)\n",
    "    miou.append(miou_loc)\n",
    "    iou.append(iou_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_files(acc, miou, iou, path):\n",
    "    d = {\n",
    "        \"acc\": [float(x) for x in acc],\n",
    "        \"iou\": [float(y) for x in iou for y in x],\n",
    "        \"miou\": [float(x) for x in miou]\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'saved_files/s2s_of.json'\n",
    "save_files(acc, miou, iou, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_acc = [np.mean(acc)]\n",
    "mean_miou = [np.mean(miou)]\n",
    "mean_iou = [np.mean(iou, axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'saved_files/s2s_of_mean.json'\n",
    "save_files(mean_acc, mean_miou, mean_iou, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
